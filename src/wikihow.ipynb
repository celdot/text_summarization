{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c011300a",
   "metadata": {
    "id": "c011300a"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5214fc38-6d0f-4b9c-8b1a-16a81c9ccfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-ignite in /opt/conda/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: torch<3,>=1.3 in /opt/conda/lib/python3.10/site-packages (from pytorch-ignite) (2.1.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytorch-ignite) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=1.3->pytorch-ignite) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n",
      "Requirement already satisfied: torcheval in /opt/conda/lib/python3.10/site-packages (0.0.7)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torcheval) (4.9.0)\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.4)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (6.1.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.10/site-packages (from plotly) (1.41.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly) (23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-ignite\n",
    "!pip install torcheval\n",
    "!pip install optuna\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dird0uRs9vR5",
   "metadata": {
    "id": "dird0uRs9vR5"
   },
   "outputs": [],
   "source": [
    "COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "IVvWsDoq9w3y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 1059,
     "status": "ok",
     "timestamp": 1748290425504,
     "user": {
      "displayName": "IN FP",
      "userId": "08561517695641011702"
     },
     "user_tz": -120
    },
    "id": "IVvWsDoq9w3y",
    "outputId": "5a12b7f2-71cc-4235-c87c-76af65d0219f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jovyan/text_summarization')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if COLAB:\n",
    "    import sys\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Mount Google Driveroo\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Add the path to the Python module\n",
    "    root_dir = '/content/drive/MyDrive/text_summarization'\n",
    "    sys.path.append(os.path.join(root_dir, 'src'))\n",
    "    sys.path.append(os.path.join(root_dir, 'src', 'utils'))\n",
    "else:\n",
    "    from pathlib import Path\n",
    "    root_dir = Path.cwd().parent\n",
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c378699a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1932,
     "status": "ok",
     "timestamp": 1748290427445,
     "user": {
      "displayName": "IN FP",
      "userId": "08561517695641011702"
     },
     "user_tz": -120
    },
    "id": "c378699a",
    "outputId": "e626591d-073b-4109-bd8e-f7bdf8f08db0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/johannes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a219a2",
   "metadata": {
    "id": "95a219a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannes/opt/anaconda3/envs/Language_Engineering_pip/lib/python3.11/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import preprocessing_pipeline, get_data_distribution\n",
    "from utils.processing import processing_pipeline, processing_pipeline_packed\n",
    "from utils.data_structures import create_packed_dataloader\n",
    "from train_model import main, tuning\n",
    "import utils.inference as inference\n",
    "from optuna.visualization import (plot_optimization_history,\n",
    "                                  plot_param_importances, plot_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66186581",
   "metadata": {
    "id": "66186581"
   },
   "outputs": [],
   "source": [
    "PROCESSING = True\n",
    "PREPROCESSING = True\n",
    "HYP_TUNING = True\n",
    "legacy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8b3c11",
   "metadata": {
    "id": "3b8b3c11"
   },
   "outputs": [],
   "source": [
    "name = \"WikiHow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae3fc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/johannes/Studium/Master/6. Semester/Language engineering/Project/text_summarization/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '/Users/johannes/Studium/Master/6. Semester/Language engineering/Project/text_summarization/'\n",
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dddaa8e0",
   "metadata": {
    "id": "dddaa8e0"
   },
   "outputs": [],
   "source": [
    "raw_dir = os.path.join(root_dir, \"raw_data\", name)\n",
    "if legacy:\n",
    "    dataset_dir = os.path.join(root_dir, \"data\", name)\n",
    "    figures_dir = os.path.join(root_dir, \"figures\", name)\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    os.makedirs(figures_dir, exist_ok=True)\n",
    "else:\n",
    "    dataset_dir = os.path.join(root_dir, \"data_packed\", name)\n",
    "    figures_dir = os.path.join(root_dir, \"figures_packed\", name)\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    os.makedirs(figures_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ffb00",
   "metadata": {
    "id": "4d1ffb00"
   },
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a38208",
   "metadata": {
    "id": "c3a38208"
   },
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(os.path.join(raw_dir, \"wikihowSep.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db627eaf",
   "metadata": {
    "id": "db627eaf"
   },
   "outputs": [],
   "source": [
    "csv_name = \"wikihow_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e12c1",
   "metadata": {
    "id": "e31e12c1"
   },
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549c6283",
   "metadata": {
    "id": "549c6283"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannes/Studium/Master/6. Semester/Language engineering/Project/text_summarization/src/utils/preprocessing.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = start_token + df[column_name] + end_token\n",
      "/Users/johannes/Studium/Master/6. Semester/Language engineering/Project/text_summarization/src/utils/preprocessing.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'length_{column_name}'] = df[column_name].apply(lambda x: len(x.split()))\n",
      "/Users/johannes/Studium/Master/6. Semester/Language engineering/Project/text_summarization/src/utils/preprocessing.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = start_token + df[column_name] + end_token\n",
      "/Users/johannes/Studium/Master/6. Semester/Language engineering/Project/text_summarization/src/utils/preprocessing.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'length_{column_name}'] = df[column_name].apply(lambda x: len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "if PREPROCESSING:\n",
    "    subset_size = 0.2 # Change later back to 50%\n",
    "    preprocessing_pipeline(dataset_df, stopwords, dataset_dir, csv_name, subset_size = subset_size, start_token = \"SOS \", end_token = \" EOS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31f09257",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5335,
     "status": "ok",
     "timestamp": 1748290468515,
     "user": {
      "displayName": "IN FP",
      "userId": "08561517695641011702"
     },
     "user_tz": -120
    },
    "id": "31f09257",
    "outputId": "eda4d71c-8c70-4912-9e2f-24a19e659e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 269126 entries, 0 to 269125\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   summary         269126 non-null  object\n",
      " 1   text            269126 non-null  object\n",
      " 2   length_summary  269126 non-null  int64 \n",
      " 3   length_text     269126 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 8.2+ MB\n"
     ]
    }
   ],
   "source": [
    "if legacy:\n",
    "    dataset_df = pd.read_csv(os.path.join(root_dir, \"data\", name, f'{csv_name}.csv'))\n",
    "else:\n",
    "    dataset_df = pd.read_csv(os.path.join(root_dir, \"data_packed\", name, f'{csv_name}.csv'))\n",
    "    \n",
    "dataset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "kSWsLCeWKqcg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1748290468527,
     "user": {
      "displayName": "IN FP",
      "userId": "08561517695641011702"
     },
     "user_tz": -120
    },
    "id": "kSWsLCeWKqcg",
    "outputId": "c2daf0b4-dd08-4f23-b148-f3109c9e9c67"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "      <th>length_summary</th>\n",
       "      <th>length_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOS put yourself out there EOS</td>\n",
       "      <td>SOS its simpleif you dont put yourself into ne...</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SOS hold your finger down to open closing opti...</td>\n",
       "      <td>SOS on some devices running dolphin you can al...</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SOS combine beeswax and turpentine to create a...</td>\n",
       "      <td>SOS mix 1 part beeswax with 3 parts turpentine...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SOS let up on the clutch while pushing down on...</td>\n",
       "      <td>SOS in order to get moving lift your left foot...</td>\n",
       "      <td>13</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SOS locate the song you would like to rate EOS</td>\n",
       "      <td>SOS using the scroll wheel locate the song tha...</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  \\\n",
       "0                     SOS put yourself out there EOS   \n",
       "1  SOS hold your finger down to open closing opti...   \n",
       "2  SOS combine beeswax and turpentine to create a...   \n",
       "3  SOS let up on the clutch while pushing down on...   \n",
       "4     SOS locate the song you would like to rate EOS   \n",
       "\n",
       "                                                text  length_summary  \\\n",
       "0  SOS its simpleif you dont put yourself into ne...               6   \n",
       "1  SOS on some devices running dolphin you can al...              10   \n",
       "2  SOS mix 1 part beeswax with 3 parts turpentine...              10   \n",
       "3  SOS in order to get moving lift your left foot...              13   \n",
       "4  SOS using the scroll wheel locate the song tha...              10   \n",
       "\n",
       "   length_text  \n",
       "0           40  \n",
       "1           31  \n",
       "2           22  \n",
       "3          172  \n",
       "4           15  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15abfd00",
   "metadata": {
    "id": "15abfd00"
   },
   "source": [
    "## Get distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb8f0f01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "executionInfo": {
     "elapsed": 8582,
     "status": "ok",
     "timestamp": 1748290477113,
     "user": {
      "displayName": "IN FP",
      "userId": "08561517695641011702"
     },
     "user_tz": -120
    },
    "id": "cb8f0f01",
    "outputId": "476bc6e5-ac3f-46e8-920b-f1eab97b9466"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannes/Studium/Master/6. Semester/Language engineering/Project/text_summarization/src/utils/preprocessing.py:99: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "get_data_distribution(dataset_df, figures_dir, \"wikihow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596a84bd",
   "metadata": {
    "id": "596a84bd"
   },
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a564bcf3",
   "metadata": {
    "id": "a564bcf3"
   },
   "outputs": [],
   "source": [
    "load_tokenizer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4ef07-990c-4b19-906b-ca0c75c4e833",
   "metadata": {
    "id": "90a837db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of text: 243\n",
      "Max length of summary: 30\n",
      "SOS token index: 1\n",
      "EOS token index: 2\n",
      "UNK token index: 3\n",
      "PAD token index: 0\n"
     ]
    }
   ],
   "source": [
    "if PROCESSING and legacy:\n",
    "    print('legacy used')\n",
    "    processing_pipeline(dataset_dir, csv_name, load_tokenizer = load_tokenizer)\n",
    "elif PROCESSING and not legacy:\n",
    "    processing_pipeline_packed(dataset_dir, csv_name, load_tokenizer=load_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024c6ff-96de-466d-83e1-f7b14d280c67",
   "metadata": {},
   "source": [
    "### Test the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33f75936-b37b-4668-8f04-5869ec579b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decoding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2362a-b656-413e-b6a7-663d3bc9311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bq/ht4rq0pn2yqbdhn46m4mt1d00000gn/T/ipykernel_20254/1400085865.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X_train = torch.load(os.path.join(dataset_dir, \"x_train.pt\"))\n",
      "/var/folders/bq/ht4rq0pn2yqbdhn46m4mt1d00000gn/T/ipykernel_20254/1400085865.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_train = torch.load(os.path.join(dataset_dir, \"y_train.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: SOS you can stay up all night dont be the first one asleep thats just dull and you miss out on all the gossip but dont be the last one either everyone will be mad at you for keeping them awake be 2nd or 3rd EOS\n",
      "Target: SOS drink some soda EOS\n",
      "-----------------------------------\n",
      "Input: SOS if your parents are open to you doing extra work around the house or some other task to earn your phone stay on top of your extra chores without being asked this will show your parents that you are taking your agreement seriously which will result in your parents taking it seriously as well EOS\n",
      "Target: SOS make a deal with your parents EOS\n",
      "-----------------------------------\n",
      "Input: SOS youll see this icon in the topright corner of the chrome browser EOS\n",
      "Target: SOS click EOS\n",
      "-----------------------------------\n",
      "Input: SOS know how much you are willing to take and dont be afraid to draw the boundary somewhere even if you want to be a patient understanding person it is perfectly okay to admit to things that make you uncomfortable or unhappy figuring out how much you can take before losing your cool can help defuse a situation before it turns explosive youre entitled to your own personal space and if others are invading it you do have the right to protect that space even drawing mental boundaries can help you determine when to take a stand and when to let things slide and the more willing you are to take a stand for yourself the less likely you are to clash with the person you find yourself at odds with EOS\n",
      "Target: SOS set boundaries EOS\n",
      "-----------------------------------\n",
      "Input: SOS you can do this by tapping open in the play store or the green icon with a winking chat bubble in the app drawer EOS\n",
      "Target: SOS open bitmoji EOS\n",
      "-----------------------------------\n",
      "Input: SOS the less people know about you the more theyll wonder if you could actually be evil note that if you do this youll likely be a pretty lonely person even if you think you want to be lonely and you want to push people away consider your reasons for doing this if youre unhappy you owe it to yourself to get help rather than making the unhappiness worse EOS\n",
      "Target: SOS be mysterious EOS\n",
      "-----------------------------------\n",
      "Input: SOS introduce yourself by name unless any further info is needed immediately leave everything else unmentioned remember unlike a job interview theres no rush here to advertise all of your best qualities right up front allow the other person to learn about you organically through a giveandtake conversation if the situation calls for it add a relevant fact to further identify yourself when you first meet likehi im the birthday boys best friend hi im your daughters in the same class as my son hi im your brother and i work in the same office EOS\n",
      "Target: SOS keep your introduction simple EOS\n",
      "-----------------------------------\n",
      "Input: SOS powders and lotions may reduce perspiration on the skin but should only be used outside the cast so that the skin stays clean and soft powder inside a cast can cake and cause sores if your cast has a perspiration odor this is normal however if you notice an unusual or foul odor coming from the cast contact your doctor EOS\n",
      "Target: SOS limit your use of powder or lotions EOS\n",
      "-----------------------------------\n",
      "Input: SOS a misshapen hat displays an overall lack of concern for style and etiquette which is why hat racks were a common feature of 19th and early 20th century buildingskeep your hat stored in a cool dry location where nothing can fall on top of it laying items on top of the hat can cause the shape of the hat to warp which can be difficult to correct if you have a hat that features a brim that is turned down store the hat upside down this relieves the pressure on the brim allowing it to maintain its form avoid handling your hat by the crown too often this can alter the hats shape over a long period of time instead handle your hat by the brim using both hands EOS\n",
      "Target: SOS store and handle your hat carefully EOS\n",
      "-----------------------------------\n",
      "Input: SOS most water heaters are supplied by voltages that can cause shock burns and even death should an energized conductor come in contact with the body shut off power in the electrical panel by removing fuses or by moving the handle of a switch or circuit breaker dedicated to the water heater to the off position completely remove and pocket the fuses or otherwise secure or lock the panel and place a note on the cover to alert everyone that work is being performed on the water heater circuit this will prevent the accidental energizing of the circuit while you are working on it EOS\n",
      "Target: SOS shut off the power EOS\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "if test_decoding:\n",
    "    import torch\n",
    "    import pickle\n",
    "    import random\n",
    "    \n",
    "    def decode_data(text_ids, index2word, EOS_token):\n",
    "        \"\"\"\n",
    "        Converts the text ids to words using the index2word mapping.\n",
    "        \"\"\"\n",
    "        if text_ids.dim() > 1:\n",
    "            text_ids = text_ids.view(-1)  # Flatten to 1D\n",
    "    \n",
    "        decoded_words = []\n",
    "        for idx in text_ids:\n",
    "            # Ensure idx is a scalar\n",
    "            if isinstance(idx, torch.Tensor):\n",
    "                idx = idx.item()\n",
    "            if idx == EOS_token:\n",
    "                decoded_words.append('EOS')\n",
    "                break\n",
    "            decoded_words.append(index2word.get(idx, 'UNK'))\n",
    "    \n",
    "        return \" \".join(decoded_words)\n",
    "    \n",
    "    if legacy:\n",
    "        X_train = torch.load(os.path.join(dataset_dir, \"x_train.pt\"))\n",
    "        y_train = torch.load(os.path.join(dataset_dir, \"y_train.pt\"))\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X_train, y_train),\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            )\n",
    "    else:\n",
    "        X_train = torch.load(os.path.join(dataset_dir, \"x_train_list.pt\"))\n",
    "        y_train = torch.load(os.path.join(dataset_dir, \"y_train_list.pt\"))\n",
    "        train_dataloader = create_packed_dataloader(X_train, y_train, pad_id=0, batch_size=1, shuffle=False)\n",
    "    \n",
    "    \n",
    "    with open(os.path.join(dataset_dir, 'feature_tokenizer.pickle'), 'rb') as handle:\n",
    "            feature_tokenizer = pickle.load(handle)\n",
    "    EOS_token = feature_tokenizer.word2index.get(\"EOS\", 2)\n",
    "    \n",
    "    nb_decoding_test = 10\n",
    "    count_test = 0\n",
    "    random_list = random.sample(range(len(train_dataloader)), nb_decoding_test)\n",
    "    if legacy:\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            if i in random_list:\n",
    "                input_tensor, target_tensor = data\n",
    "                print('Input: {}'.format(decode_data(input_tensor[0], feature_tokenizer.index2word, EOS_token)))\n",
    "                print('Target: {}'.format(decode_data(target_tensor[0], feature_tokenizer.index2word, EOS_token)))\n",
    "                print('-----------------------------------')\n",
    "                count_test += 1\n",
    "            if count_test == nb_decoding_test:\n",
    "                break\n",
    "    else:\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            if i in random_list:\n",
    "                input_tensor, input_lengths, target_tensor, target_lengths = data\n",
    "                print('Input: {}'.format(decode_data(input_tensor[0], feature_tokenizer.index2word, EOS_token)))\n",
    "                print('Target: {}'.format(decode_data(target_tensor[0], feature_tokenizer.index2word, EOS_token)))\n",
    "                print('-----------------------------------')\n",
    "                count_test += 1\n",
    "            if count_test == nb_decoding_test:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b32c9d",
   "metadata": {
    "id": "17b32c9d"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f42f2",
   "metadata": {
    "id": "db7f42f2"
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "max_length = 100\n",
    "lr = 0.001\n",
    "weight_decay = 1e-6\n",
    "batch_size = 128\n",
    "num_workers = 2\n",
    "n_epochs = 100\n",
    "print_example_every = 10\n",
    "load_checkpoint = False\n",
    "early_stopping_patience = 5\n",
    "log_every = 2000\n",
    "\n",
    "optimizer_hyperparams = {\n",
    "    'learning_rate': lr,\n",
    "    'weight_decay': weight_decay,\n",
    "    'n_epochs': n_epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': num_workers,\n",
    "    'early_stopping_patience': early_stopping_patience\n",
    "}\n",
    "\n",
    "model_hyperparams = {\n",
    "    'hidden_size': hidden_size,\n",
    "    'max_length': max_length\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde23ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "executionInfo": {
     "elapsed": 977047,
     "status": "error",
     "timestamp": 1748294670175,
     "user": {
      "displayName": "IN FP",
      "userId": "08561517695641011702"
     },
     "user_tz": -120
    },
    "id": "7bde23ac",
    "outputId": "b5ef08c6-afff-40e7-a72c-49936dcaf55f"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "main() missing 1 required positional argument: 'log_every'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_hyperparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_hyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtuning\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtuning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_hyperparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_hyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_examples_every\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_example_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlegacy\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: main() missing 1 required positional argument: 'log_every'"
     ]
    }
   ],
   "source": [
    "main(root_dir = root_dir,\n",
    "    model_hyperparams=model_hyperparams,\n",
    "    tuning = tuning, \n",
    "    optimizer_hyperparams=optimizer_hyperparams,\n",
    "    log_every = log_every,\n",
    "    print_examples_every=print_example_every,\n",
    "    load_checkpoint=load_checkpoint,\n",
    "    name=name,\n",
    "    legacy=legacy\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0617b5c-6156-4ee1-a78a-cb59fbc2f243",
   "metadata": {},
   "source": [
    "### Making inference with the model (on a CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c633a-f75d-4142-bae2-933a08c54671",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'best_checkpoint.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585a80d-a92f-435a-b6e3-015eea7b9f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    input_tensor = input(\"Enter the text to summarize (or type 'exit' to quit): \")\n",
    "    if input_tensor.lower() == 'exit':\n",
    "        break\n",
    "    # Don't forget to water your plants, they need it to survive.\n",
    "    inference.main(root_dir, name, checkpoint_name, hidden_size, max_length, input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c090fe-4407-4a3c-b6f2-f47d6836f31a",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09efad8-2f38-4175-9b97-069ce8f46420",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c77807-8a8a-4171-b806-cd5a9ea9f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYP_TUNING:\n",
    "    study = tuning(root_dir, num_trials, name)\n",
    "    # Save the study results\n",
    "    study_dir = os.path.join(root_dir, 'parameters_tuning', name, 'study_results')\n",
    "    os.makedirs(study_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the optimization history\n",
    "    plot_optimization_history(study)\n",
    "    \n",
    "    # Save the parameter importances\n",
    "    plot_param_importances(study)\n",
    "    \n",
    "    # Save the slice plot\n",
    "    plot_slice(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f806a-4f1a-4702-81f3-1ad73a08bfd4",
   "metadata": {},
   "source": [
    "### Check training information with tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d951d2-ca13-47a6-b99b-27730cefe1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# !tensorboard --logdir='tensorboard_logs'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Language_Engineering_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
